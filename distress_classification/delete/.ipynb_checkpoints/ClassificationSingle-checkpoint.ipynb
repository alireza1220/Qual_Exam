{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRACK Clssification using GAPS DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#import readdata\n",
    "from tensorflow import keras\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 6-classes 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the gaps v2 ZEB256\n",
      "max tr = 25, max va = 5, max te = 5\n",
      "low-load data in process\n",
      "current: tr:2, va:1, te:1\n",
      "loading 1 chunk  out of 2 of training in v2 gaps\n",
      "loading 2 chunk  out of 2 of training in v2 gaps\n",
      "loading 1 chunk  out of 1 of valid in v2 gaps\n",
      "loading 1 chunk  out of 1 of test in v2 gaps\n",
      "data name  is: gaps v2 ZEB256\n",
      "input shape is : (256, 256, 3)\n",
      "train num is : 4000\n",
      "valid num is : 2000\n",
      "test  num is : 2000\n",
      "number of classes are : 6\n"
     ]
    }
   ],
   "source": [
    "from importlib.machinery import SourceFileLoader\n",
    "MODULENAME = \"loadgaps\"\n",
    "MODULEPATH = \"/home/ali/my_project/large_files/gaps/loadgaps.py\"\n",
    "lgaps = SourceFileLoader(MODULENAME, MODULEPATH).load_module()\n",
    "\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = lgaps.loadv2_ZEB256(load = 'low')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v1 binary 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': '2CCP_1FDD', '2': 'DUPLICATE', '3': 'Transfer_learning_vgg'}\n"
     ]
    }
   ],
   "source": [
    "model_archive = {\n",
    "                '1' : '2CCP_1FDD',\n",
    "                '2' : 'DUPLICATE',\n",
    "                '3' : 'Transfer_learning_vgg'\n",
    "                    }\n",
    "print(model_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last change\n",
      "X current model is: model_1_2CCP_1FDD\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 254, 254, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 254, 254, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 127, 127, 64)      18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 127, 127, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 125, 125, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 125, 125, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 246016)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               125960704 \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 126,029,350\n",
      "Trainable params: 126,029,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import readmodel\n",
    "model_name = 'model_1' # 'model_1', 'model_2', or model_3\n",
    "input_shape = x_train[0,:,:,:].shape\n",
    "num_classes = y_train.shape[1]\n",
    "model = readmodel.modelchoose(model_name,input_shape, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPILE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "categorical_crossentropy\n"
     ]
    }
   ],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.01, decay=1e-6)\n",
    "\n",
    "print(num_classes)\n",
    "# Let's train the model using RMSprop\n",
    "if num_classes == 2:\n",
    "    loss = 'binary_crossentropy'\n",
    "if num_classes == 6:\n",
    "    loss = 'categorical_crossentropy'\n",
    "    \n",
    "print(loss)\n",
    "\n",
    "model.compile( loss = loss, # loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## GENERATE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5 # 200\n",
    "batch_size = 50\n",
    "#num_classes = y_train_binary.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT DATA TO MODEL using IMG GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model on batches with real-time data augmentation:\n",
    "history = model.fit_generator(datagen.flow(x_train, y_train_binary, batch_size=batch_size),\n",
    "                    epochs=epochs, \n",
    "                    steps_per_epoch=len(x_train) / batch_size,\n",
    "                    validation_data=(x_valid, y_valid_binary),\n",
    "                    shuffle=True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIT DATA to MODEL w/o IMG GENRATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "4000/4000 [==============================] - 36s 9ms/sample - loss: 5388.5378 - accuracy: 0.5688 - val_loss: 1.2164 - val_accuracy: 0.5920\n",
      "Epoch 2/5\n",
      "4000/4000 [==============================] - 32s 8ms/sample - loss: 1.3841 - accuracy: 0.5945 - val_loss: 1.2133 - val_accuracy: 0.5920\n",
      "Epoch 3/5\n",
      "4000/4000 [==============================] - 32s 8ms/sample - loss: 1.1882 - accuracy: 0.6095 - val_loss: 1.2123 - val_accuracy: 0.5920\n",
      "Epoch 4/5\n",
      "4000/4000 [==============================] - 32s 8ms/sample - loss: 1.1864 - accuracy: 0.6097 - val_loss: 1.2164 - val_accuracy: 0.5920\n",
      "Epoch 5/5\n",
      "4000/4000 [==============================] - 32s 8ms/sample - loss: 1.1895 - accuracy: 0.6093 - val_loss: 1.2144 - val_accuracy: 0.5920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history_2 = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_valid, y_valid),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/1 - 5s - loss: 1.0146 - accuracy: 0.6045\n",
      "Test loss: 1.159772804260254\n",
      "Test accuracy: 0.6045\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACCURACY & VAL vs EPOCHS PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb6d53c46a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE MODEL and WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_1_epochs_5_batchsize_50.h5\n"
     ]
    }
   ],
   "source": [
    "model_weight_name = model_name + f'_epochs_{epochs}_batchsize_{batch_size}.h5'\n",
    "print(model_weight_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ali/my_project/gaps'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at /home/ali/my_project/gaps/saved_models/model_1_epochs_5_batchsize_50.h5 \n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "#model_name = 'gaps_model_1.h5'\n",
    "model_weight_name = model_name + f'_epochs_{epochs}_batchsize_{batch_size}.h5'\n",
    "\n",
    "#os.getcwd()\n",
    "# save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "save_dir = '/home/ali/my_project/gaps/saved_models'\n",
    "model_path = os.path.join(save_dir, model_weight_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRID SEARCH FIT for H_PARAM TUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model(model_name):\n",
    "    #model_name = 'model_3' # 'model_1', 'model_2', 'model_3'\n",
    "    model = readmodel.modelchoose(model_name,input_shape, num_classes)\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "modelC = KerasClassifier(build_fn=create_model, verbose=2)\n",
    "\n",
    "# using grid search\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.01, decay=1e-6)\n",
    "\n",
    "epochs = [2]\n",
    "model_name = ['model_1', 'model_3']\n",
    "\n",
    "param_grid = dict(model_name = model_name)\n",
    "\n",
    "grid = GridSearchCV(estimator = modelC , param_grid=param_grid)\n",
    "grid_result = grid.fit(x_train, y_train_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRID SEARCH RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = grid_result.best_estimator_.model.history\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['loss'], label = 'loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
